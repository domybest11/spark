#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
# spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
# spark.driver.memory              5g
# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"

spark.maxRemoteBlockSizeFetchToMem 512m
spark.hadoopRDD.ignoreEmptySplits  true

spark.yarn.archive              viewfs://jssz-bigdata-cluster/data/spark/production/jars/spark-2.4-20200218
spark.yarn.dist.files           viewfs://jssz-bigdata-cluster/data/spark/production/jars/log4j.properties
spark.master                     yarn-client
spark.submit.deployMode          client
spark.debug.maxToStringFields     1000

#event log
spark.eventLog.enabled            false
spark.eventLog.compress           false
spark.eventLog.dir                viewfs://jssz-bigdata-cluster/tmp/sparklog
spark.history.fs.logDirectory     viewfs://jssz-bigdata-cluster/tmp/sparklog

spark.yarn.historyServer.address  http://10.69.1.34:18080
spark.history.fs.cleaner.enabled  true
spark.history.fs.cleaner.maxAge   7d

spark.driver.extraJavaOptions -XX:-TieredCompilation -XX:SurvivorRatio=8 -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:OnOutOfMemoryError='kill -9 %p'  -verbose.gc -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCDateStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintTenuringDistribution -XX:+UnlockExperimentalVMOptions -XX:G1LogLevel=finest -Xloggc:/data/log/spark2.4/spark-thriftserver-gc.log -XX:+PrintGCDetails

#spark.driver.extraJavaOptions -XX:-TieredCompilation -XX:SurvivorRatio=8 -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:OnOutOfMemoryError='kill -9 %p'  -verbose.gc -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCDateStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintTenuringDistribution -XX:+UnlockExperimentalVMOptions -XX:G1LogLevel=finest -Xloggc:/data/log/spark2.4/spark-thriftserver-gc.log -XX:+PrintGCDetails  -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=1190 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -javaagent:/data/service/sparklib/jmx_prometheus_javaagent-0.11.0.jar=1191:/data/service/spark2.4/conf/spark-thriftserver.yaml
spark.executor.extraJavaOptions -XX:SurvivorRatio=8 -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:OnOutOfMemoryError='kill -9 %p'

spark.ui.retainedJobs 100
spark.ui.retainedStages 500
spark.ui.retainedTasks 50000
spark.ui.dagGraph.retainedRootRDDs 2
spark.history.retainedApplications 10
spark.deploy.retainedApplications 10
spark.deploy.retainedDrivers  10
spark.streaming.ui.retainedBatches 50
spark.sql.thriftserver.ui.retainedSessions 100
spark.sql.thriftserver.ui.retainedStatements 100
spark.sql.ui.retainedExecutions 10
spark.sql.shuffle.partitions       1000

spark.ui.retainedDeadExecutors 10
spark.ui.timeline.tasks.maximum 50
spark.ui.timeline.executors.maximum 50
spark.history.ui.maxApplications 2000

spark.yarn.queue      root.common.adhoc

spark.sql.autoBroadcastJoinThreshold    25000000
spark.sql.orc.filterPushdown true
spark.sql.shuffle.partitions 1000
spark.sql.autoBroadcastJoinThreshold 25000000
spark.sql.hive.metastorePartitionPruning true
spark.sql.statistics.fallBackToHdfs true
spark.sql.parquet.binaryAsString true
spark.sql.hive.convertMetastoreParquet false
spark.sql.hive.convertMetastoreOrc false
spark.sql.adaptive.enabled true
spark.sql.adaptive.minNumPostShufflePartitions 50
#spark.sql.adaptive.shuffle.targetPostShuffleInputSize 134217728
#spark.sql.exchange.reuse false
spark.sql.hive.caseSensitiveInferenceMode NEVER_INFER
#该参数可能会导致有的目录读不到数据,如果不开启的话，如果分区对应的hdfs目录不存在，就会报错
spark.sql.hive.verifyPartitionPath true
spark.broadcast.blockSize 16m
spark.sql.crossJoin.enabled true
spark.sql.objectHashAggregate.sortBased.fallbackThreshold 512
spark.sql.windowExec.buffer.spill.threshold 2000000
spark.sql.sortMergeJoinExec.buffer.spill.threshold 2000000
spark.sql.cartesianProductExec.buffer.spill.threshold 2000000

spark.local.dir /mnt/storage00/spark31logs
spark.ui.killEnabled true
spark.scheduler.mode    FAIR
spark.rpc.message.maxSize 256
spark.serializer org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer 64m
spark.kryoserializer.buffer.max 512m
spark.driver.maxResultSize 0
spark.io.compression.codec lz4
spark.hadoop.validateOutputSpecs false
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 2
spark.hadoop.dfs.client.hedged.read.threadpool.size 3
spark.hadoop.dfs.client.hedged.read.threshold.millis 10000
spark.blacklist.enabled false
#spark.scheduler.listenerbus.eventqueue.size 100000
spark.scheduler.listenerbus.eventqueue.capacity 100000
spark.port.maxRetries   100
spark.ui.port 10005


spark.rpc.lookupTimeout 120s
spark.rpc.askTimeout 120s
spark.network.timeout 600s

spark.locality.wait.process 1ms
spark.locality.wait.node 3ms
spark.locality.wait.rack 30ms

spark.driver.memory        25g
spark.executor.memory          6g
spark.executor.cores           4
#spark.yarn.executor.memoryOverhead 2048
spark.executor.memoryOverhead   4096
spark.dynamicAllocation.enabled true
spark.shuffle.service.enabled   true
spark.dynamicAllocation.initialExecutors 10
spark.dynamicAllocation.minExecutors     2
spark.dynamicAllocation.maxExecutors     400
spark.dynamicAllocation.executorIdleTimeout 15s


#eleme feature
spark.proxyuser.enabled false
spark.sql.hive.loadFunctionResource true
spark.hive.auth.enable false
spark.sql.hive.dropTableIgnoreIfNotExists true
spark.yarn.exitWhenYarnApplicationExit true
spark.driver.allowExitWhenContextStop false
spark.sql.hive.mergeFiles true
spark.sql.hive.merge.smallfile.size 120000000
spark.sql.hive.merge.size.per.task  240000000
spark.sql.default.fileformat orcfile
spark.sql.hive.groupingid.enabled false
spark.shuffle.spill.limit 20g
spark.shuffle.useOldFetchProtocol  true
spark.rdd.parallelPartitionsThreshold 2900


spark.shuffle.service.port         7339

#kafka conf
spark.acks 3
spark.batch.size 20000
spark.buffer.memory 33554432
spark.linger.ms 500
spark.max.request.size 1048576
spark.request.timeout.ms 50000
spark.retries 3
spark.bootstrap.servers 172.22.33.94:9092,172.22.33.99:9092,172.22.33.97:9092
spark.key.serializer org.apache.kafka.common.serialization.StringSerializer
spark.value.serializer org.apache.kafka.common.serialization.ByteArraySerializer

