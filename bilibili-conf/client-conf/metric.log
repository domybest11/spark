Java HotSpot(TM) 64-Bit Server VM warning: Cannot open file /data/log/spark2.4/spark-thriftserver-gc.log due to No such file or directory

Listening for transport dt_socket at address: 5020
21/04/26 20:19:54 [main] INFO ProducerConfig: ProducerConfig values: 
	acks = 0
	batch.size = 20000
	bootstrap.servers = [172.22.33.94:9092, 172.22.33.99:9092, 172.22.33.97:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 50000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

21/04/26 20:19:54 [main] INFO AppInfoParser: Kafka version: 2.6.0
21/04/26 20:19:54 [main] INFO AppInfoParser: Kafka commitId: 62abe01bee039651
21/04/26 20:19:54 [main] INFO AppInfoParser: Kafka startTimeMs: 1619439594994
21/04/26 20:19:55 [main] WARN SparkConf: The configuration key 'spark.blacklist.enabled' has been deprecated as of Spark 3.1.0 and may be removed in the future. Please use spark.excludeOnFailure.enabled
21/04/26 20:19:55 [main] WARN SparkConf: The configuration key 'spark.maxRemoteBlockSizeFetchToMem' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.
21/04/26 20:19:55 [main] WARN SparkConf: The configuration key 'spark.maxRemoteBlockSizeFetchToMem' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.
21/04/26 20:19:55 [main] WARN SparkConf: The configuration key 'spark.blacklist.enabled' has been deprecated as of Spark 3.1.0 and may be removed in the future. Please use spark.excludeOnFailure.enabled
21/04/26 20:19:55 [main] WARN SparkConf: The configuration key 'spark.maxRemoteBlockSizeFetchToMem' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.
21/04/26 20:19:55 [main] WARN SparkConf: The configuration key 'spark.blacklist.enabled' has been deprecated as of Spark 3.1.0 and may be removed in the future. Please use spark.excludeOnFailure.enabled
21/04/26 20:19:55 [main] WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/04/26 20:19:56 [main] WARN SparkConf: The configuration key 'spark.blacklist.enabled' has been deprecated as of Spark 3.1.0 and may be removed in the future. Please use spark.excludeOnFailure.enabled
21/04/26 20:19:56 [main] WARN SparkConf: The configuration key 'spark.maxRemoteBlockSizeFetchToMem' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.
21/04/26 20:19:56 [main] INFO HiveConf: Found configuration file file:/data/src/spark-3.1.1/conf/hive-site.xml
21/04/26 20:19:57 [main] WARN HiveConf: HiveConf of name hive.exec.orc.split.strategy.etl.max.files does not exist
21/04/26 20:19:57 [main] WARN HiveConf: HiveConf of name hive.support.sql11.reserved.keywords does not exist
21/04/26 20:19:57 [main] WARN HiveConf: HiveConf of name hive.mapred.supports.subdirectories does not exist
21/04/26 20:19:57 [main] WARN HiveConf: HiveConf of name hive.exec.orc.split.strategy.etl.max.files does not exist
21/04/26 20:19:57 [main] WARN HiveConf: HiveConf of name hive.auth.enable does not exist
21/04/26 20:19:57 [main] WARN HiveConf: HiveConf of name hive.support.sql11.reserved.keywords does not exist
21/04/26 20:19:57 [main] WARN HiveConf: HiveConf of name hive.mapred.supports.subdirectories does not exist
21/04/26 20:19:57 [main] INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('viewfs://jssz-bigdata-cluster/warehouse1').
21/04/26 20:19:57 [main] INFO SharedState: Warehouse path is 'viewfs://jssz-bigdata-cluster/warehouse1'.
21/04/26 20:19:57 [main] INFO log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
21/04/26 20:19:57 [main] INFO log: Conf is not init.
21/04/26 20:19:57 [main] INFO log: get counter max: 120
21/04/26 20:19:58 [main] WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
21/04/26 20:19:58 [main] INFO ConfiguredFailoverProxyProvider: Failing over to /10.69.8.34:9000
21/04/26 20:19:58 [main] INFO SessionState: Created HDFS directory: /tmp/hive/hive/10423dbb-5131-42c1-aa10-e5cffd7026b5
21/04/26 20:19:58 [main] INFO SessionState: Created local directory: /tmp/root/10423dbb-5131-42c1-aa10-e5cffd7026b5
21/04/26 20:19:58 [main] INFO SessionState: Created HDFS directory: /tmp/hive/hive/10423dbb-5131-42c1-aa10-e5cffd7026b5/_tmp_space.db
21/04/26 20:19:58 [main] WARN SparkConf: The configuration key 'spark.blacklist.enabled' has been deprecated as of Spark 3.1.0 and may be removed in the future. Please use spark.excludeOnFailure.enabled
21/04/26 20:19:58 [main] WARN SparkConf: The configuration key 'spark.maxRemoteBlockSizeFetchToMem' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.
21/04/26 20:20:05 [main] WARN SparkConf: The configuration key 'spark.blacklist.enabled' has been deprecated as of Spark 3.1.0 and may be removed in the future. Please use spark.excludeOnFailure.enabled
21/04/26 20:20:05 [main] WARN SparkConf: The configuration key 'spark.maxRemoteBlockSizeFetchToMem' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.
21/04/26 20:20:05 [main] WARN SparkConf: The configuration key 'spark.maxRemoteBlockSizeFetchToMem' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.
21/04/26 20:20:05 [main] WARN SparkConf: The configuration key 'spark.blacklist.enabled' has been deprecated as of Spark 3.1.0 and may be removed in the future. Please use spark.excludeOnFailure.enabled
21/04/26 20:20:05 [main] INFO SparkContext: Running Spark version 3.1.1
21/04/26 20:20:05 [main] WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
21/04/26 20:20:05 [main] INFO ResourceUtils: ==============================================================
21/04/26 20:20:05 [main] INFO ResourceUtils: No custom resources configured for spark.driver.
21/04/26 20:20:05 [main] INFO ResourceUtils: ==============================================================
21/04/26 20:20:05 [main] INFO SparkContext: Submitted application: a_h_q_5311804502_33455851
21/04/26 20:20:05 [main] INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 4096, script: , vendor: , cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 6144, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/04/26 20:20:05 [main] INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor
21/04/26 20:20:05 [main] INFO ResourceProfileManager: Added ResourceProfile id: 0
21/04/26 20:20:05 [main] INFO ProducerConfig: ProducerConfig values: 
	acks = 0
	batch.size = 262144
	bootstrap.servers = [10.69.179.17:9092, 10.69.179.18:9092, 10.69.179.19:9092, 10.69.179.20:9092, 10.69.181.30:9092, 10.69.181.31:9092, 10.69.181.32:9092, 10.69.181.33:9092, 10.70.38.11:9092, 10.70.38.12:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 50000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

21/04/26 20:20:05 [main] INFO AppInfoParser: Kafka version: 2.6.0
21/04/26 20:20:05 [main] INFO AppInfoParser: Kafka commitId: 62abe01bee039651
21/04/26 20:20:05 [main] INFO AppInfoParser: Kafka startTimeMs: 1619439605590
21/04/26 20:20:05 [kafka-producer-network-thread | producer-2] INFO Metadata: [Producer clientId=producer-2] Cluster ID: AOeFaonJRoS4kSfIugin0w
21/04/26 20:20:05 [main] INFO SecurityManager: Changing view acls to: root,hive
21/04/26 20:20:05 [main] INFO SecurityManager: Changing modify acls to: root,hive
21/04/26 20:20:05 [main] INFO SecurityManager: Changing view acls groups to: 
21/04/26 20:20:05 [main] INFO SecurityManager: Changing modify acls groups to: 
21/04/26 20:20:05 [main] INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root, hive); groups with view permissions: Set(); users  with modify permissions: Set(root, hive); groups with modify permissions: Set()
21/04/26 20:20:06 [main] INFO Utils: Successfully started service 'sparkDriver' on port 26095.
21/04/26 20:20:06 [main] INFO SparkEnv: Registering MapOutputTracker
21/04/26 20:20:06 [main] INFO SparkEnv: Registering BlockManagerMaster
21/04/26 20:20:06 [main] INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/04/26 20:20:06 [main] INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/04/26 20:20:06 [main] INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/04/26 20:20:06 [main] INFO DiskBlockManager: Created local directory at /mnt/storage00/spark24logs/blockmgr-720cd264-9314-491e-ae24-cc1496c9e477
21/04/26 20:20:06 [main] INFO MemoryStore: MemoryStore started with capacity 14.7 GiB
21/04/26 20:20:06 [main] INFO SparkEnv: Registering OutputCommitCoordinator
21/04/26 20:20:06 [main] INFO log: Logging initialized @51922ms to org.sparkproject.jetty.util.log.Slf4jLog
21/04/26 20:20:06 [main] INFO Server: jetty-9.4.36.v20210114; built: 2021-01-14T16:44:28.689Z; git: 238ec6997c7806b055319a6d11f8ae7564adc0de; jvm 1.8.0_162-b12
21/04/26 20:20:06 [main] INFO Server: Started @52040ms
21/04/26 20:20:06 [main] WARN Utils: Service 'SparkUI' could not bind on port 10005. Attempting port 10006.
21/04/26 20:20:06 [main] INFO AbstractConnector: Started ServerConnector@2530cb90{HTTP/1.1, (http/1.1)}{0.0.0.0:10006}
21/04/26 20:20:06 [main] INFO Utils: Successfully started service 'SparkUI' on port 10006.
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4ca0b9b1{/jobs,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4ca970d5{/jobs/json,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6a2badb1{/jobs/job,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6cee903a{/jobs/job/json,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2c2aab92{/stages,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@10a0d93a{/stages/json,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@66b31d46{/stages/stage,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@29aa4bc9{/stages/stage/json,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6c519e47{/stages/pool,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4768b95c{/stages/pool/json,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7573c7b5{/storage,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@717a8a76{/storage/json,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@97b5e6a{/storage/rdd,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@c754401{/storage/rdd/json,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4e7eba9f{/environment,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@b841713{/environment/json,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4fee14b{/executors,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3a54638b{/executors/json,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@e886caf{/executors/threadDump,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1884e671{/executors/threadDump/json,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5158a9f7{/static,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@70022d44{/,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5abbb273{/api,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@36871e98{/jobs/job/kill,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6533629{/stages/stage/kill,null,AVAILABLE,@Spark}
21/04/26 20:20:06 [main] INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://jssz-bigdata-test-11.host.bilibili.co:10006
21/04/26 20:20:06 [main] INFO FairSchedulableBuilder: Creating Fair Scheduler pools from default file: fairscheduler.xml
21/04/26 20:20:06 [main] INFO FairSchedulableBuilder: Created pool: default, schedulingMode: FAIR, minShare: 2, weight: 1
21/04/26 20:20:06 [main] INFO HadoopDelegationTokenManager: Attempting to load user's ticket cache.
21/04/26 20:20:06 [main] INFO HadoopFSDelegationTokenProvider: getting token for: org.apache.hadoop.fs.viewfs.ViewFileSystem@1a336906 with renewer rm/jssz-bigdata-namenode-05.host.bilibili.co@BILIBILI.CO
21/04/26 20:20:06 [main] INFO ConfiguredFailoverProxyProvider: Failing over to /10.69.8.34:9000
21/04/26 20:20:06 [main] INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 68902831 for hive on ha-hdfs:jssz-bigdata-ns1
21/04/26 20:20:06 [main] INFO ConfiguredFailoverProxyProvider: Failing over to /10.69.1.31:9000
21/04/26 20:20:06 [main] INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 326763704 for hive on ha-hdfs:jssz-bigdata-ns2
21/04/26 20:20:06 [main] INFO HadoopFSDelegationTokenProvider: getting token for: org.apache.hadoop.fs.viewfs.ViewFileSystem@1a336906 with renewer hive/jssz-bigdata-test-11.host.bilibili.co@BILIBILI.CO
21/04/26 20:20:06 [main] INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 68902832 for hive on ha-hdfs:jssz-bigdata-ns1
21/04/26 20:20:06 [main] INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 326763705 for hive on ha-hdfs:jssz-bigdata-ns2
21/04/26 20:20:06 [main] INFO SparkHadoopUtil: Updating delegation tokens for current user.
21/04/26 20:20:06 [main] INFO Utils: Using initial executors = 1, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
21/04/26 20:20:07 [main] INFO Client: Requesting a new application from cluster with 2416 NodeManagers
21/04/26 20:20:07 [main] INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (28672 MB per container)
21/04/26 20:20:07 [main] INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
21/04/26 20:20:07 [main] INFO Client: Setting up container launch context for our AM
21/04/26 20:20:07 [main] INFO Client: Setting up the launch environment for our AM container
21/04/26 20:20:07 [main] INFO Client: Preparing resources for our AM container
21/04/26 20:20:07 [main] WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
21/04/26 20:20:08 [main] INFO Client: Uploading resource file:/mnt/storage00/spark24logs/spark-6364605f-80ad-45d4-92fc-9b1ab7b23f07/__spark_libs__980327976129825719.zip -> viewfs://jssz-bigdata-cluster/user/hive/.sparkStaging/application_1619429960338_20342/__spark_libs__980327976129825719.zip
21/04/26 20:20:11 [main] INFO Client: Uploading resource file:/mnt/storage00/spark24logs/spark-6364605f-80ad-45d4-92fc-9b1ab7b23f07/__spark_conf__3802328385768069975.zip -> viewfs://jssz-bigdata-cluster/user/hive/.sparkStaging/application_1619429960338_20342/__spark_conf__.zip
21/04/26 20:20:11 [main] INFO SecurityManager: Changing view acls to: root,hive
21/04/26 20:20:11 [main] INFO SecurityManager: Changing modify acls to: root,hive
21/04/26 20:20:11 [main] INFO SecurityManager: Changing view acls groups to: 
21/04/26 20:20:11 [main] INFO SecurityManager: Changing modify acls groups to: 
21/04/26 20:20:11 [main] INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root, hive); groups with view permissions: Set(); users  with modify permissions: Set(root, hive); groups with modify permissions: Set()
21/04/26 20:20:11 [main] INFO Client: Submitting application application_1619429960338_20342 to ResourceManager
21/04/26 20:20:11 [main] INFO YarnClientImpl: Submitted application application_1619429960338_20342
21/04/26 20:20:11 [main] INFO Client: root队列CPU资源使用率75%(130035 / 172426), 内存资源使用率100%(529498112 / 530007040)。集群比较忙碌，请耐心等待！
21/04/26 20:20:11 [main] INFO Client: 当前job执行队列root.report.adhocCPU资源已用26，等待的资源需求0, 内存资源已用0.08TB，等待的资源需求0.00TB, 当前队列等待任务：0, <a href='http://luckbear.bilibili.co/#/resource/compute?queue=root.report.adhoc&user=hive/jssz-bigdata-test-11.host.bilibili.co@BILIBILI.CO' target='_blank' style='color:#409EFF'>查看详情</a>
21/04/26 20:20:11 [main] INFO Client: 部门队列root.reportCPU资源已用19782，等待的资源需求0, 内存资源已用64.21TB，等待的资源需求0.00TB
21/04/26 20:20:12 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:12 [main] INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: root.report.adhoc
	 start time: 1619439611375
	 final status: UNDEFINED
	 tracking URL: http://jssz-bigdata-namenode-05.host.bilibili.co:8088/proxy/application_1619429960338_20342/
	 user: hive
21/04/26 20:20:13 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:14 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:15 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:16 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:17 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:18 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:19 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:20 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:21 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:22 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:23 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:24 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:25 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:26 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:27 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:28 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:29 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:30 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:31 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:32 [main] INFO Client: Application report for application_1619429960338_20342 (state: ACCEPTED)
21/04/26 20:20:33 [main] INFO Client: Application report for application_1619429960338_20342 (state: RUNNING)
21/04/26 20:20:33 [main] INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 10.69.97.13
	 ApplicationMaster RPC port: -1
	 queue: root.report.adhoc
	 start time: 1619439611375
	 final status: UNDEFINED
	 tracking URL: http://jssz-bigdata-namenode-05.host.bilibili.co:8088/proxy/application_1619429960338_20342/
	 user: hive
21/04/26 20:20:33 [main] INFO YarnClientSchedulerBackend: Application application_1619429960338_20342 has started running.
21/04/26 20:20:33 [main] INFO YarnScheduler: Starting speculative execution thread
21/04/26 20:20:33 [main] INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3797.
21/04/26 20:20:33 [main] INFO NettyBlockTransferService: Server created on jssz-bigdata-test-11.host.bilibili.co:3797
21/04/26 20:20:33 [main] INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/04/26 20:20:33 [main] INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, jssz-bigdata-test-11.host.bilibili.co, 3797, None)
21/04/26 20:20:33 [dispatcher-BlockManagerMaster] INFO BlockManagerMasterEndpoint: Registering block manager jssz-bigdata-test-11.host.bilibili.co:3797 with 14.7 GiB RAM, BlockManagerId(driver, jssz-bigdata-test-11.host.bilibili.co, 3797, None)
21/04/26 20:20:33 [main] INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, jssz-bigdata-test-11.host.bilibili.co, 3797, None)
21/04/26 20:20:33 [main] INFO BlockManager: external shuffle service port = 7339
21/04/26 20:20:33 [main] INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, jssz-bigdata-test-11.host.bilibili.co, 3797, None)
21/04/26 20:20:33 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@58a7a58d{/metrics/json,null,AVAILABLE,@Spark}
21/04/26 20:20:33 [main] INFO Utils: Using initial executors = 1, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
21/04/26 20:20:33 [dispatcher-event-loop-8] WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!
21/04/26 20:20:34 [dispatcher-event-loop-16] INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> jssz-bigdata-namenode-05.host.bilibili.co,jssz-bigdata-namenode-06.host.bilibili.co, PROXY_URI_BASES -> http://jssz-bigdata-namenode-05.host.bilibili.co:8088/proxy/application_1619429960338_20342,http://jssz-bigdata-namenode-06.host.bilibili.co:8088/proxy/application_1619429960338_20342, RM_HA_URLS -> null,null), /proxy/application_1619429960338_20342
21/04/26 20:20:41 [dispatcher-event-loop-19] INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
21/04/26 20:20:54 [dispatcher-CoarseGrainedScheduler] INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.69.212.13:2806) with ID 1,  ResourceProfileId 0
21/04/26 20:20:54 [spark-listener-group-executorManagement] INFO ExecutorMonitor: New executor 1 has registered (new total is 1)
21/04/26 20:20:54 [main] INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.2
21/04/26 20:20:55 [dispatcher-BlockManagerMaster] INFO BlockManagerMasterEndpoint: Registering block manager jssz-bigdata-datanode-863.host.bilibili.co:3353 with 3.3 GiB RAM, BlockManagerId(1, jssz-bigdata-datanode-863.host.bilibili.co, 3353, None)
21/04/26 20:20:56 [main] INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('viewfs://jssz-bigdata-cluster/warehouse1').
21/04/26 20:20:56 [main] INFO SharedState: Warehouse path is 'viewfs://jssz-bigdata-cluster/warehouse1'.
21/04/26 20:20:56 [main] INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
21/04/26 20:20:56 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2ff8d39b{/SQL,null,AVAILABLE,@Spark}
21/04/26 20:20:56 [main] INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
21/04/26 20:20:56 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@69d902f9{/SQL/json,null,AVAILABLE,@Spark}
21/04/26 20:20:56 [main] INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
21/04/26 20:20:56 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@20723ee{/SQL/execution,null,AVAILABLE,@Spark}
21/04/26 20:20:56 [main] INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
21/04/26 20:20:56 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2b0d85bd{/SQL/execution/json,null,AVAILABLE,@Spark}
21/04/26 20:20:56 [main] INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
21/04/26 20:20:56 [main] INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1ed12d10{/static/sql,null,AVAILABLE,@Spark}
21/04/26 20:20:56 [main] INFO ProducerConfig: ProducerConfig values: 
	acks = 0
	batch.size = 262144
	bootstrap.servers = [10.69.179.17:9092, 10.69.179.18:9092, 10.69.179.19:9092, 10.69.179.20:9092, 10.69.181.30:9092, 10.69.181.31:9092, 10.69.181.32:9092, 10.69.181.33:9092, 10.70.38.11:9092, 10.70.38.12:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 50000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

21/04/26 20:20:56 [main] INFO AppInfoParser: Kafka version: 2.6.0
21/04/26 20:20:56 [main] INFO AppInfoParser: Kafka commitId: 62abe01bee039651
21/04/26 20:20:56 [main] INFO AppInfoParser: Kafka startTimeMs: 1619439656069
21/04/26 20:20:56 [kafka-producer-network-thread | producer-3] INFO Metadata: [Producer clientId=producer-3] Cluster ID: AOeFaonJRoS4kSfIugin0w
21/04/26 20:20:56 [main] INFO ProducerConfig: ProducerConfig values: 
	acks = 0
	batch.size = 262144
	bootstrap.servers = [10.69.179.17:9092, 10.69.179.18:9092, 10.69.179.19:9092, 10.69.179.20:9092, 10.69.181.30:9092, 10.69.181.31:9092, 10.69.181.32:9092, 10.69.181.33:9092, 10.70.38.11:9092, 10.70.38.12:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.type = gzip
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 50000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

21/04/26 20:20:56 [main] INFO AppInfoParser: Kafka version: 2.6.0
21/04/26 20:20:56 [main] INFO AppInfoParser: Kafka commitId: 62abe01bee039651
21/04/26 20:20:56 [main] INFO AppInfoParser: Kafka startTimeMs: 1619439656091
21/04/26 20:20:56 [kafka-producer-network-thread | producer-4] INFO Metadata: [Producer clientId=producer-4] Cluster ID: AOeFaonJRoS4kSfIugin0w
21/04/26 20:20:56 [main] WARN SQLConf: The SQL config 'spark.sql.hive.verifyPartitionPath' has been deprecated in Spark v3.0 and may be removed in the future. This config is replaced by 'spark.files.ignoreMissingFiles'.
21/04/26 20:20:56 [main] WARN SQLConf: The SQL config 'spark.sql.hive.verifyPartitionPath' has been deprecated in Spark v3.0 and may be removed in the future. This config is replaced by 'spark.files.ignoreMissingFiles'.
21/04/26 20:20:56 [main] WARN SQLConf: The SQL config 'spark.sql.hive.verifyPartitionPath' has been deprecated in Spark v3.0 and may be removed in the future. This config is replaced by 'spark.files.ignoreMissingFiles'.
21/04/26 20:20:57 [main] WARN SQLConf: The SQL config 'spark.sql.hive.verifyPartitionPath' has been deprecated in Spark v3.0 and may be removed in the future. This config is replaced by 'spark.files.ignoreMissingFiles'.
21/04/26 20:20:57 [main] INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.0 using Spark classes.
21/04/26 20:20:57 [main] INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is viewfs://jssz-bigdata-cluster/warehouse1
21/04/26 20:20:57 [main] INFO metastore: Trying to connect to metastore with URI thrift://10.68.69.21:9083
21/04/26 20:20:57 [main] INFO metastore: Opened a connection to metastore, current connections: 1
21/04/26 20:20:57 [main] INFO metastore: Connected to metastore.
Spark master: yarn, Application Id: application_1619429960338_20342
21/04/26 20:20:57 [main] INFO SparkSQLCLIDriver: Spark master: yarn, Application Id: application_1619429960338_20342
21/04/26 20:20:57 [main] ERROR SparkSQLCLIDriver: Could not open input file for reading. (File file:/data/src/spark-3.1.1/conf/sql.sql does not exist)
log4j:WARN driver-logError-task is stoping ......
21/04/26 20:20:57 [Async metric sink] INFO HttpUtils: http util send post request code is  200
21/04/26 20:20:57 [Async metric sink] INFO HttpUtils: http util send post request success 
21/04/26 20:20:57 [pool-1-thread-1] INFO AbstractConnector: Stopped Spark@2530cb90{HTTP/1.1, (http/1.1)}{0.0.0.0:10006}
21/04/26 20:20:57 [pool-1-thread-1] INFO SparkUI: Stopped Spark web UI at http://jssz-bigdata-test-11.host.bilibili.co:10006
21/04/26 20:20:57 [YARN application state monitor] INFO YarnClientSchedulerBackend: Interrupting monitor thread
21/04/26 20:20:57 [pool-1-thread-1] INFO YarnClientSchedulerBackend: Shutting down all executors
21/04/26 20:20:57 [dispatcher-CoarseGrainedScheduler] INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
21/04/26 20:20:57 [pool-1-thread-1] INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
21/04/26 20:20:57 [pool-1-thread-1] INFO KafkaProducer: [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 10000 ms.
21/04/26 20:20:57 [dispatcher-event-loop-0] INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/04/26 20:20:57 [pool-1-thread-1] INFO MemoryStore: MemoryStore cleared
21/04/26 20:20:57 [pool-1-thread-1] INFO BlockManager: BlockManager stopped
21/04/26 20:20:57 [pool-1-thread-1] INFO BlockManagerMaster: BlockManagerMaster stopped
21/04/26 20:20:57 [dispatcher-event-loop-8] INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/04/26 20:20:57 [pool-1-thread-1] INFO SparkContext: Successfully stopped SparkContext
21/04/26 20:20:57 [refresh configuration-0] INFO RefreshConfigThread: refresh config thread interrupted 
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.spark.util.RefreshConfigThread.run(ConfigurationUtil.scala:196)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/04/26 20:20:57 [pool-1-thread-1] INFO SqlAppStatusScheduler: Invoking stopMetricThread from shutdown hook
21/04/26 20:20:57 [pool-1-thread-1] INFO SqlAppStatusScheduler: sql metric thread is stoped.
21/04/26 20:20:57 [pool-1-thread-1] INFO ShutdownHookManager: Shutdown hook called
21/04/26 20:20:57 [pool-1-thread-1] INFO ShutdownHookManager: Deleting directory /tmp/spark-bdf4ca11-de9f-4213-8175-6bbc08307627
21/04/26 20:20:57 [pool-1-thread-1] INFO ShutdownHookManager: Deleting directory /mnt/storage00/spark24logs/spark-6364605f-80ad-45d4-92fc-9b1ab7b23f07
