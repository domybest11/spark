<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!--
            Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<!-- Put site-specific property overrides in this file. -->
<configuration>

  <!-- resourcemanager configuration -->
  <property>
    <name>yarn.resourcemanager.ha.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.resourcemanager.cluster-id</name>
    <value>resource-jssz</value>
  </property>
  <property>
    <name>yarn.resourcemanager.ha.rm-ids</name>
    <value>jssz-rm1,jssz-rm2</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname.jssz-rm1</name>
    <value>10.69.1.33</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname.jssz-rm2</name>
    <value>10.69.1.34</value>
  </property>
  <property>
  <property>
    <name>yarn.resourcemanager.webapp.address.jssz-rm1</name>
    <value>127.0.0.1:8088</value>
  </property>
  <property>
    <name>yarn.resourcemanager.webapp.address.jssz-rm2</name>
    <value>127.0.0.1:8088</value>
  </property>
  <property>
    <name>yarn.resourcemanager.address.jssz-rm1</name>
    <value>10.69.1.33:8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.address.jssz-rm2</name>
    <value>10.69.1.34:8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.address.jssz-rm1</name>
    <value>10.69.1.33:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.address.jssz-rm2</name>
    <value>10.69.1.34:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address.jssz-rm1</name>
    <value>10.69.1.33:8031</value>
  </property>
    <name>yarn.resourcemanager.resource-tracker.address.jssz-rm2</name>
    <value>10.69.1.34:8031</value>
  </property>

  <!-- scheduler configuration -->
  <property>
    <name>yarn.resourcemanager.scheduler.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
  </property>
  <property>
    <name>yarn.resourcemanager.zk-address</name>
    <value>10.69.8.34:2181,10.69.1.30:2181,10.69.1.31:2181</value>
    <description>For multiple zk services, separate them with comma</description>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.monitor.enable</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.scheduler.fair.user-as-default-queue</name>
    <value>false</value>
  </property>
  <property>
    <name>yarn.acl.enable</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.nodemanager.remote-app-log-dir</name>
    <value>hdfs://jssz-bigdata-ns4/tmp/logs</value>
  </property>
  <property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value>259200</value>
  </property>
  <property>
    <name>yarn.log-aggregation.retain-check-interval-seconds</name>
    <value>300</value>
  </property>
  <property>
    <name>yarn.nodemanager.vmem-pmem-ratio</name>
    <value>2.1</value>
  </property>
  <property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>4096</value>
  </property>
  <property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>10240</value>
  </property>
  <property>
    <name>yarn.scheduler.assignmultiple</name>
    <value>false</value>
  </property>
  <property>
    <name>yarn.scheduler.fair.allow-undeclared-pools</name>
    <value>false</value>
  </property>

  <!-- nodemanager  -->
  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>225280</value>
  </property>
  <property>
    <name>yarn.nodemanager.local-dirs</name>
    <value>/mnt/storage00/logs/yarn/nm/localdir</value>
  </property>
  <property>
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>72</value>
  </property>
  <property>
    <name>yarn.scheduler.minimum-allocation-vcores</name>
    <value>2</value>
  </property>
  <property>
    <name>yarn.nodemanager.log-aggregation.compression-type</name>
    <value>none</value>
  </property>
  <property>
    <name>yarn.nodemanager.delete.debug-delay-sec</name>
    <value>600</value>
  </property>
  <property>
    <name>yarn.nodemanager.log-dirs</name>
    <value>/mnt/storage00/logs/yarn/nm/containerlogs</value>
  </property>
  <property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
  </property>
  <property>
    <name>yarn.nodemanager.delete.thread-count</name>
    <value>12</value>
  </property>

  <!-- timeline server -->
  <property>
    <name>yarn.timeline-service.enabled</name>
    <value>false</value>
  </property>
  <property>
    <name>yarn.resourcemanager.system-metrics-publisher.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.timeline-service.leveldb-timeline-store.path</name>
    <value>/mnt/storage00/logs/timeline</value>
  </property>
  <property>
    <name>yarn.timeline-service.hostname</name>
    <value>10.69.1.34</value>
  </property>
  <property>
    <name>yarn.timeline-service.http-cross-origin.enabled</name>
    <value>false</value>
  </property>
  <property>
     <name>yarn.resourcemanager.system-metrics-publisher.enabled</name>
     <value>true</value>
  </property>
  <property>
     <name>yarn.nodemanager.pmem-check-enabled</name>
     <value>false</value>
  </property>
  <property>
     <name>yarn.nodemanager.localizer.cache.target-size-mb</name>
     <value>102400</value>
  </property>
  <property>
     <name>yarn.nodemanager.localizer.cache.target-size-mb</name>
     <value>102400</value>
  </property>
  <property>
     <name>yarn.resourcemanager.am.max-attempts</name>
     <value>4</value>
   </property>
  <property>
     <name>yarn.nodemanager.pmem-check-enabled</name>
     <value>true</value>
  </property>
  <property>
     <name>yarn.scheduler.maximum-allocation-vcores</name>
     <value>30</value>
  </property>
  <property>
     <name>yarn.resourcemanager.recovery.enabled</name>
     <value>true</value>
  </property>
  <property>
     <name>yarn.resourcemanager.fs.state-store.uri</name>
     <value>/tmp/yarn/system/rmstore</value>
  </property>
  <property>
    <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
    <value>98.5</value>
  </property>
  <property>
    <name>yarn.nodemanager.address</name>
    <value>0.0.0.0:8041</value>
  </property>
  <property>
    <name>yarn.nodemanager.container-localizer.java.opts</name>
    <value>-Xmx256m -XX:ParallelGCThreads=3 -XX:CICompilerCount=2</value>
  </property>

  <!-- security config -->
  <property>
    <name>yarn.resourcemanager.keytab</name>
    <value>/etc/security/keytabs/rm.service.keytab</value>
  </property>
  <property>
    <name>yarn.resourcemanager.principal</name>
    <value>rm/_HOST@BILIBILI.CO</value>
  </property>
  <property>
    <name>yarn.nodemanager.keytab</name>
    <value>/etc/security/keytabs/nm.service.keytab</value>
  </property>
  <property>
    <name>yarn.nodemanager.principal</name>
    <value>nm/_HOST@BILIBILI.CO</value>
  </property>
  <property>
    <name>yarn.nodemanager.admin.enable</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.nodemanager.admin.address</name>
    <value>0.0.0.0:8048</value>
  </property>
  <property>
    <name>yarn.log.server.url</name>
    <value>http://jssz-bigdata-namenode-06.host.bilibili.co:19090/jobhistory/logs</value>
  </property>

  <!-- LinuxContainerExecutor config -->
  <property>
    <name>yarn.nodemanager.container-executor.class</name>
    <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.user-ensure-command</name>
    <value>/etc/hadoop-nodemanager/ensure-user.sh</value>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.group</name>
    <value>hadoop</value>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.path</name>
    <value>/usr/bin/container-executor</value>
  </property>

  <!-- NM Restart Recover -->
  <property>
    <name>yarn.nodemanager.recovery.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.nodemanager.recovery.dir</name>
    <value>/mnt/storage00/datum/yarn-nm-recovery</value>
  </property>
  <property>
    <name>yarn.nodemanager.recovery.supervised</name>
    <value>true</value>
  </property>

  <!-- NM AuxServices -->
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle,spark_shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
    <value>org.apache.spark.network.yarn.YarnShuffleService</value>
  </property>
  <property>
    <name>spark.shuffle.service.port</name>
    <value>7337</value>
  </property>
  <property>
    <name>spark.shuffle.service.index.cache.entries</name>
    <value>2048</value>
  </property>
  <property>
    <name>spark.shuffle.io.serverThreads</name>
    <value>128</value>
  </property>
  <property>
    <name>spark.shuffle.io.backLog</name>
    <value>8192</value>
  </property>
  <property>
    <name>spark.yarn.shuffle.stopOnFailure</name>
    <value>true</value>
  </property>

  <!-- support-cgroup -->
  <property>
    <name>yarn.nodemanager.linux-container-executor.resources-handler.class</name>
    <value>org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler</value>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.cgroups.mount</name>
    <value>false</value>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.cgroups.mount-path</name>
    <value>/sys/fs/cgroup</value>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.cgroups.hierarchy</name>
    <value>/hadoop-yarn</value>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage</name>
    <value>false</value>
  </property>
  <property>
    <name>yarn.nodemanager.resource.percentage-physical-cpu-limit</name>
    <value>95</value>
  </property>
  <property>
    <name>yarn.nodemanager.resource.count-logical-processors-as-cores</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled</name>
    <value>true</value>
  </property>

  <!-- container cgroup dir deleted service -->
  <property>
    <name>yarn.nodemanager.linux-container-executor.cgroups.delete-service-interval-ms</name>
    <value>600000</value>
  </property>

  <!-- support cpu cgroup hard limit dynamically -->
  <property>
    <name>yarn.nodemanager.vcpu-check-enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.nodemanager.vcpu-overlimit-ratio-threshold</name>
    <value>5.0</value>
  </property>
  <property>
    <name>yarn.nodemanager.vcpu-overlimit-nums-threshold</name>
    <value>10</value>
  </property>
  <property>
    <name>yarn.nodemanager.vcpu-sample-nums-threshold</name>
    <value>15</value>
  </property>
  <property>
    <name>yarn.nodemanager.cpu-hardlimit-ratio</name>
    <value>3</value>
  </property>
  <property>
    <name>yarn.nodemanager.cpu-hardlimit-req-queue-limit</name>
    <value>10000</value>
  </property>
  <property>
    <name>yarn.nodemanager.cpu-hardlimit-real-limit</name>
    <value>true</value>
  </property>

  <!-- shuffle disk degrade -->
  <property>
    <name>yarn.nodemanager.shuffle.disk.degrade.enble</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.nodemanager.storage.type.local-dirs</name>
    <value>/mnt/storage00/logs/yarn/nm/localdir,
      /mnt/storage01/logs/yarn/nm/localdir,
      /mnt/storage02/logs/yarn/nm/localdir,
      /mnt/storage03/logs/yarn/nm/localdir,
      /mnt/storage04/logs/yarn/nm/localdir,
      /mnt/storage05/logs/yarn/nm/localdir,
      /mnt/storage06/logs/yarn/nm/localdir,
      /mnt/storage07/logs/yarn/nm/localdir,
      /mnt/storage08/logs/yarn/nm/localdir,
      /mnt/storage09/logs/yarn/nm/localdir,
      /mnt/storage10/logs/yarn/nm/localdir,
      /mnt/storage11/logs/yarn/nm/localdir,
      /mnt/storage12/logs/yarn/nm/localdir,
      /mnt/storage13/logs/yarn/nm/localdir,
      /mnt/storage14/logs/yarn/nm/localdir,
    </value>
  </property>
  <property>
    <name>yarn.nodemanager.local-dirs-tiers</name>
    <value>011111111111111</value>
  </property>
  <property>
    <name>yarn.nodemanager.shuffle.disk.degrade.threshold</name>
    <value>90</value>
  </property>
</configuration>
