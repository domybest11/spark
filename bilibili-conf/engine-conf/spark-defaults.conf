spark.kyuubi.model.enabled true
spark.kerberos.keytab /etc/security/keytabs/hive.service.keytab
spark.kerberos.principal hive/_HOST@BILIBILI.CO
#core
spark.master                     yarn
spark.submit.deployMode          client
spark.yarn.queue       report.adhoc
spark.driver.memory        100g
spark.driver.maxResultSize  3G
spark.driver.extraJavaOptions -Dfile.encoding=utf-8 -Duser.timezone=Asia/Shanghai -XX:-TieredCompilation -XX:SurvivorRatio=8 -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:OnOutOfMemoryError='kill -9 %p'  -verbose.gc -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCDateStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintTenuringDistribution -XX:+UnlockExperimentalVMOptions -XX:G1LogLevel=finest -Xloggc:/data/log/spark3.1/spark-thriftserver-gc-%t-%p.log -XX:+PrintGCDetails -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=1190 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -javaagent:/data/service/sparklib/jmx_prometheus_javaagent-0.11.0.jar=1191:/data/service/spark3.1/conf/spark-thriftserver.yaml -Dlog4j.configuration=log4j-driver.properties
spark.executor.memory          8g
spark.executor.cores           4
spark.executor.memoryOverhead   2048
spark.executor.instances                 1
spark.executor.extraJavaOptions -Dfile.encoding=utf-8 -Duser.timezone=Asia/Shanghai -XX:SurvivorRatio=8 -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:OnOutOfMemoryError='kill -9 %p' -Dlog4j.configuration=log4j-executor.properties -verbose.gc -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+PrintGCDetails -XX:+PrintGCCause -XX:+PrintTenuringDistribution -XX:+PrintFlagsFinal
spark.executor.processTreeMetrics.enabled true
spark.dynamicAllocation.enabled true
spark.dynamicAllocation.initialExecutors 20
spark.dynamicAllocation.minExecutors     20
spark.dynamicAllocation.maxExecutors     1200
spark.dynamicAllocation.executorIdleTimeout 60s
spark.dynamicAllocation.executorAllocationRatio 1
spark.deploy.retainedApplications 10
spark.deploy.retainedDrivers  10
spark.broadcast.blockSize 100m
spark.debug.maxToStringFields     1000
spark.scheduler.mode    FAIR
spark.scheduler.maxRegisteredResourcesWaitingTime 30m
spark.scheduler.minRegisteredResourcesRatio 0.2
spark.scheduler.listenerbus.eventqueue.capacity 100000
spark.yarn.report.interval 10s
spark.excludeOnFailure.enabled false
spark.port.maxRetries   100
spark.retries 3
spark.maxRemoteBlockSizeFetchToMem 512m
spark.blacklist.enabled false
spark.python.worker.memory                      6g
spark.max.request.size 10485760
spark.request.timeout.ms 50000
spark.batch.size 262144
spark.buffer.memory 33554432
spark.max.allow.taskcount.inOneStage 300000 
spark.resultGetter.threads 20
spark.stage.running.task.limit.enable true
spark.stage.running.task.limit.count 1000
spark.stage.running.task.limitRatio  0.2
spark.excludeOnFailure.enabled true

#rpc
spark.rpc.retry.wait 3s
spark.rpc.numRetries 6
spark.rpc.io.backLog 128
spark.rpc.lookupTimeout 120s
spark.rpc.askTimeout 240s
spark.yarn.askTimeout 480s
spark.network.timeout 240s
spark.rpc.message.maxSize 256
spark.yarn.am.memory 1024m

#shuffle
spark.shuffle.service.enabled   true
spark.shuffle.service.port         7338
spark.shuffle.spill.limit 20g
spark.shuffle.useOldFetchProtocol  true
spark.shuffle.file.buffer 64k
spark.shuffle.io.maxRetries 5
spark.shuffle.io.retryWait 6s
spark.shuffle.registration.timeout 5000
spark.shuffle.registration.maxAttempts 30
spark.network.maxRemoteBlockSizeFetchToMem 512m
spark.reducer.maxBlocksInFlightPerAddress 1000
spark.rpc.io.enableTcpKeepAlive true
spark.io.compression.codec zstd
spark.io.compression.zstd.bufferSize      512k
spark.io.compression.zstd.level              1

spark.hadoopRDD.ignoreEmptySplits  true
spark.rdd.parallelPartitionsThreshold 1000000
spark.rdd.parallelListingThreshold 1000000
spark.serializer org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer 64m
spark.kryoserializer.buffer.max 512m
spark.hadoop.validateOutputSpecs false
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 2
spark.hadoop.mapreduce.input.fileinputformat.split.maxsize 268435456

spark.locality.wait.process 1ms
spark.locality.wait.node 3ms
spark.locality.wait.rack 30ms
spark.metrics.sink.used.max.resource true

# spark.speculation
spark.speculation true
spark.speculation.interval 10000ms
spark.speculation.multiplier 4
spark.speculation.quantile 0.9
spark.speculation.min.time 10000ms

#sql
spark.sql.sources.partitionOverwriteMode dynamic
spark.sql.legacy.timeParserPolicy LEGACY
spark.sql.autoDetectDataSkewEnabled true
spark.sql.autoDetectDataSkewMultiplier 5
spark.sql.storeAssignmentPolicy LEGACY
spark.sql.broadcast.fallBackToSMJ true
spark.sql.broadcastTimeout 900
spark.sql.autoBroadcastJoinThreshold 25000000
spark.sql.adaptive.enabled true
spark.sql.adaptive.coalescePartitions.minPartitionNum 50
spark.sql.adaptive.advisoryPartitionSizeInBytes 128MB
spark.sql.combine.hive.input.splits.enabled true
spark.sql.crossJoin.enabled true
spark.sql.cartesianProductExec.buffer.spill.threshold 2000000
spark.sql.hive.convertMetastoreParquet true
spark.sql.hive.convertMetastoreOrc true
spark.sql.hive.caseSensitiveInferenceMode NEVER_INFER
spark.sql.hive.metastorePartitionPruning true
spark.sql.hive.mergeFiles true
spark.sql.hive.merge.smallfile.size 120000000
spark.sql.hive.merge.size.per.task  240000000
#该参数可能会导致有的目录读不到数据,如果不开启的话，如果分区对应的hdfs目录不存在，就会报错
spark.sql.hive.verifyPartitionPath false
spark.files.ignoreMissingFiles true
spark.sql.legacy.createHiveTableByDefault false
spark.sql.orc.filterPushdown true
spark.sql.objectHashAggregate.sortBased.fallbackThreshold 512
spark.sql.parquet.binaryAsString true
spark.sql.sources.default orc
spark.sql.default.fileformat orcfile
spark.sql.hive.groupingid.enabled false
spark.sql.shuffle.partitions 1000
spark.sql.statistics.fallBackToHdfs true
spark.sql.sortMergeJoinExec.buffer.spill.threshold 2000000
spark.sql.windowExec.buffer.spill.threshold 2000000
spark.sql.files.maxPartitionBytes 200MB
spark.sql.queryExecutionListeners com.bilibili.lineage.spark.LineageQueryListener
spark.sql.partitionStats.enabled true



spark.yarn.archive               hdfs://jssz-bigdata-ns4/data/spark/production/jars/spark-3.1-20211027-114104.zip
spark.yarn.dist.files            viewfs://jssz-bigdata-cluster/data/spark/production/log4j/log4j.properties
spark.yarn.historyServer.address  10.71.190.31:18081
#spark.yarn.stagingDir   hdfs://jssz-bigdata-ns7/user

#event log
spark.local.dir /mnt/storage00/spark31logs
spark.eventLog.enabled            false
spark.eventLog.compress           false
spark.eventLog.dir                viewfs://jssz-bigdata-cluster/tmp/sparklog
spark.history.fs.logDirectory     viewfs://jssz-bigdata-cluster/tmp/sparklog
spark.history.fs.cleaner.enabled  true
spark.history.fs.cleaner.maxAge   7d
spark.history.retainedApplications 10
spark.history.ui.maxApplications 2000
spark.ui.retainedDeadExecutors 2000
spark.ui.timeline.tasks.maximum 50
spark.ui.timeline.executors.maximum 50
spark.ui.killEnabled true
spark.ui.port 10005


#bilibili上报相关
spark.failureJobCollector.enabled true
spark.linger.ms 500
spark.traceReporter.enabled true
spark.traceReporter.logId 007464
spark.lancer.url http://dataflow.biliapi.com/log/system
spark.bootstrap.servers 10.69.179.17:9092,10.69.179.18:9092,10.69.179.19:9092,10.69.179.20:9092,10.69.181.30:9092,10.69.181.31:9092,10.69.181.32:9092,10.69.181.33:9092,10.70.38.11:9092,10.70.38.12:9092
spark.yarn.appMasterEnv.CONF_USE_CONF true
spark.yarn.appMasterEnv.CONF_VERSION spark-sink
spark.yarn.appMasterEnv.CONF_HOST config.bilibili.co
spark.yarn.appMasterEnv.CONF_PATH /tmp/spark-conf
spark.yarn.appMasterEnv.CONF_RELOAD false
spark.yarn.appMasterEnv.CONF_TOKEN 231ef1c5882e85157fb2300cf695bf7c
spark.yarn.appMasterEnv.TREE_ID 143974
spark.yarn.appMasterEnv.DEPLOY_ENV prod
spark.yarn.appMasterEnv.SPRING_CONFIG_LOCATION /tmp/spark
spark.yarn.appMasterEnv.ZONE sz001



#eleme feature
spark.proxyuser.enabled true
spark.files  /etc/security/keytabs/spark.service.keytab
spark.executorEnv.EXECUTOR_PROXY_USER_ENABLE true
spark.executorEnv.EXECUTOR_PRINCIPAL hive/_HOST@BILIBILI.CO
spark.executorEnv.EXECUTOR_KEYTAB spark.service.keytab
spark.sql.hive.loadFunctionResource true
spark.hive.auth.enable false
spark.sql.hive.dropTableIgnoreIfNotExists true
spark.yarn.exitWhenYarnApplicationExit true
spark.driver.allowExitWhenContextStop false


# iceberg configurations
spark.sql.extensions             org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.iceberg        org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.iceberg.type   hive

spark.sql.catalog.spark_catalog  org.apache.iceberg.spark.SparkSessionCatalog
spark.sql.catalog.spark_catalog.type hive

spark.shuffle.disk.degrade.enabled true
spark.deploy.autoConf true
spark.sql.app.listener.enabled true