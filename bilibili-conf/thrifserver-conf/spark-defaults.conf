spark.metrics.sink.used.max.resource true
spark.executor.processTreeMetrics.enabled true

spark.sql.autoDetectDataSkewEnabled true
spark.sql.autoDetectDataSkewMultiplier 5

spark.report.logicalplan  false
spark.logicalplan.topic   r_bdp_platform.ods_meta_lineage_sql_spark
spark.failureJobCollector.enabled true
spark.yarn.report.interval 10s
spark.traceReporter.enabled true
spark.traceReporter.logId 007464
spark.lancer.url http://dataflow.biliapi.com/log/system

spark.scheduler.maxRegisteredResourcesWaitingTime 30m
spark.sql.broadcastTimeout 900
spark.scheduler.minRegisteredResourcesRatio 0.2

spark.rdd.parallelListingThreshold 1000000
spark.thriftserver.model.enabled true
spark.sql.broadcast.fallBackToSMJ true
spark.sql.combine.hive.input.splits.enabled true
spark.hadoop.mapreduce.input.fileinputformat.split.maxsize 268435456

spark.maxRemoteBlockSizeFetchToMem 512m
spark.hadoopRDD.ignoreEmptySplits  true
spark.yarn.archive               hdfs://jssz-bigdata-ns4/data/spark/production/jars/spark-3.1-20210512.zip
spark.yarn.dist.files            viewfs://jssz-bigdata-cluster/data/spark/production/log4j/log4j.properties
spark.master                     yarn
spark.submit.deployMode          client
spark.debug.maxToStringFields     1000


spark.yarn.appMasterEnv.CONF_USE_CONF true
spark.yarn.appMasterEnv.CONF_VERSION spark-sink
spark.yarn.appMasterEnv.CONF_HOST config.bilibili.co
spark.yarn.appMasterEnv.CONF_PATH /tmp/spark-conf
spark.yarn.appMasterEnv.CONF_RELOAD false
spark.yarn.appMasterEnv.CONF_TOKEN 231ef1c5882e85157fb2300cf695bf7c
spark.yarn.appMasterEnv.TREE_ID 143974
spark.yarn.appMasterEnv.DEPLOY_ENV prod
spark.yarn.appMasterEnv.SPRING_CONFIG_LOCATION /tmp/spark
spark.yarn.appMasterEnv.ZONE sz001

#event log
spark.eventLog.enabled            false
spark.eventLog.compress           false
spark.eventLog.dir                viewfs://jssz-bigdata-cluster/tmp/sparklog
spark.history.fs.logDirectory     viewfs://jssz-bigdata-cluster/tmp/sparklog

spark.yarn.historyServer.address  http://10.69.1.34:18080
spark.history.fs.cleaner.enabled  true
spark.history.fs.cleaner.maxAge   7d

spark.driver.extraJavaOptions -XX:-TieredCompilation -XX:SurvivorRatio=8 -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:OnOutOfMemoryError='kill -9 %p'  -verbose.gc -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCDateStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintTenuringDistribution -XX:+UnlockExperimentalVMOptions -XX:G1LogLevel=finest -Xloggc:/data/log/spark3.1/spark-thriftserver-gc-%t-%p.log -XX:+PrintGCDetails -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=1190 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -javaagent:/data/service/sparklib/jmx_prometheus_javaagent-0.11.0.jar=1191:/data/service/spark3.1/conf/spark-thriftserver.yaml -Dlog4j.configuration=log4j-driver.properties
spark.executor.extraJavaOptions -XX:SurvivorRatio=8 -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:OnOutOfMemoryError='kill -9 %p' -Dlog4j.configuration=log4j-executor.properties

spark.history.retainedApplications 10
spark.deploy.retainedApplications 10
spark.deploy.retainedDrivers  10

spark.sql.shuffle.partitions       1000

spark.ui.retainedDeadExecutors 2000
spark.ui.timeline.tasks.maximum 50
spark.ui.timeline.executors.maximum 50
spark.history.ui.maxApplications 2000

#spark.yarn.queue      root.common.adhoc
spark.yarn.queue       report.adhoc
spark.sql.autoBroadcastJoinThreshold    25000000
spark.sql.orc.filterPushdown true
spark.sql.shuffle.partitions 1000
spark.sql.autoBroadcastJoinThreshold 25000000
spark.sql.hive.metastorePartitionPruning true
spark.sql.statistics.fallBackToHdfs true
spark.sql.parquet.binaryAsString true
spark.sql.hive.convertMetastoreParquet false
spark.sql.hive.convertMetastoreOrc false
spark.sql.adaptive.enabled true
spark.sql.adaptive.coalescePartitions.minPartitionNum 50
spark.sql.adaptive.advisoryPartitionSizeInBytes 128MB
spark.local.dir /mnt/storage00/spark31logs
spark.sql.hive.caseSensitiveInferenceMode NEVER_INFER
#该参数可能会导致有的目录读不到数据,如果不开启的话，如果分区对应的hdfs目录不存在，就会报错
spark.sql.hive.verifyPartitionPath false
spark.broadcast.blockSize 16m
spark.sql.crossJoin.enabled true
spark.sql.objectHashAggregate.sortBased.fallbackThreshold 512
spark.sql.windowExec.buffer.spill.threshold 2000000
spark.sql.sortMergeJoinExec.buffer.spill.threshold 2000000
spark.sql.cartesianProductExec.buffer.spill.threshold 2000000

spark.local.dir /data/log/spark31logs
spark.ui.killEnabled true
spark.scheduler.mode    FAIR
spark.rpc.message.maxSize 256
spark.serializer org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer 64m
spark.kryoserializer.buffer.max 512m
spark.io.compression.codec zstd
spark.io.compression.zstd.bufferSize      512k
spark.io.compression.zstd.level              1
spark.hadoop.validateOutputSpecs false
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 2
spark.blacklist.enabled false

spark.scheduler.listenerbus.eventqueue.capacity 100000
spark.port.maxRetries   100
spark.ui.port 10005


spark.rpc.lookupTimeout 120s
spark.rpc.askTimeout 120s
spark.network.timeout 600s

spark.locality.wait.process 1ms
spark.locality.wait.node 3ms
spark.locality.wait.rack 30ms

spark.driver.memory        10g
spark.executor.memory          6g
spark.executor.cores           3
#spark.yarn.executor.memoryOverhead 2048
spark.executor.memoryOverhead   2048
spark.dynamicAllocation.enabled true
spark.shuffle.service.enabled   true
spark.dynamicAllocation.initialExecutors 1
spark.dynamicAllocation.minExecutors     1
spark.executor.instances                 1
spark.dynamicAllocation.maxExecutors     400
spark.dynamicAllocation.executorIdleTimeout 60s
spark.dynamicAllocation.executorAllocationRatio 0.2
spark.driver.maxResultSize                      512m
spark.python.worker.memory                      6g

# spark.speculation
spark.speculation true
spark.speculation.interval 10000ms
spark.speculation.multiplier 4
spark.speculation.quantile 0.9
spark.speculation.min.time 10000ms


#eleme feature
spark.proxyuser.enabled true
spark.files  /etc/security/keytabs/hive.keytab
spark.executorEnv.EXECUTOR_PROXY_USER_ENABLE true
spark.executorEnv.EXECUTOR_PRINCIPAL hive@BILIBILI.CO
spark.executorEnv.EXECUTOR_KEYTAB hive.keytab
spark.sql.hive.loadFunctionResource true
spark.hive.auth.enable false
spark.sql.hive.dropTableIgnoreIfNotExists true
spark.yarn.exitWhenYarnApplicationExit true
spark.driver.allowExitWhenContextStop false
spark.sql.hive.mergeFiles true
spark.sql.hive.merge.smallfile.size 120000000
spark.sql.hive.merge.size.per.task  240000000
spark.sql.default.fileformat orcfile
spark.sql.hive.groupingid.enabled false
spark.shuffle.spill.limit 20g
spark.shuffle.useOldFetchProtocol  true
spark.rdd.parallelPartitionsThreshold 1000000

spark.shuffle.service.port         7338
spark.batch.size 262144
spark.buffer.memory 33554432
spark.linger.ms 500
spark.max.request.size 10485760
spark.request.timeout.ms 50000
spark.retries 3
spark.bootstrap.servers 10.69.179.17:9092,10.69.179.18:9092,10.69.179.19:9092,10.69.179.20:9092,10.69.181.30:9092,10.69.181.31:9092,10.69.181.32:9092,10.69.181.33:9092,10.70.38.11:9092,10.70.38.12:9092
